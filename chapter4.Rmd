# RStudio Exercise 4

## 2. Loading data (Boston)

```{r}
# access the MASS package
library(MASS)

# load the data
data("Boston")

# Structure and dimensions of the data
str(Boston)
```
Original data description can be found [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html).

The Boston data frame is of housing values in suburbs of Boston and it has 506 rows and 14 columns.

**crim**      per capita crime rate by town.  
**zn**        proportion of residential land zoned for lots over 25,000 sq.ft.  
**indus**     proportion of non-retail business acres per town.  
**chas**      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).  
**nox**       nitrogen oxides concentration (parts per 10 million).  
**rm**        average number of rooms per dwelling.  
**age**       proportion of owner-occupied units built prior to 1940.  
**dis**       weighted mean of distances to five Boston employment centres.  
**rad**       index of accessibility to radial highways.  
**tax**       full-value property-tax rate per \$10,000.  
**ptratio**   pupil-teacher ratio by town.  
**black**     1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.  
**lstat**     lower status of the population (percent).  
**medv**      median value of owner-occupied homes in \$1000s.  

## 3. Graphical overview and summaries of the data

```{r}
summary(Boston)
```

Plot matrix of variables

```{r}
#fig1, fig.height = 19, fig.width = 17}
library(ggplot2)
library(GGally)

# remove chas variable which is categorical
Boston_cont <- subset(Boston, select = -chas)

ggpairs(Boston_cont,
        upper = list(continuous = wrap('cor', size = 4)),
        title = "Scatterplot matrix of `Boston`")
```

Very few variable seems to be truely normally distributed. **rm** (average number of rooms per dwelling) variable seem to be the closest.

Let's take a more graphical look at the correlations of the variables:

```{r}
#fig.height = 12, fig.width = 8}
library(corrplot)

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)
```

In the correlation matrix visualization above the more red color symbolizes negative correlation the bigger it is. The positive correlation is presented by blue tint and the bigger the circle, bigger the (positive).

_Few picks on the correlation matrix:_
**crim** "per capita crime rate by town" is most positively correlated with **rad** "index of accessibility to radial highways". 

*indus* "proportion of non-retail business acres per town." is negatively correlated with *dis* "weighted mean of distances to five Boston employment centres". So... bigger proportion of non-retail business acres per town is situated away from Boston's five employment centers, in other words, industy areas are situated away from center's of the towns. 

## 4. Standardize dataset

Standardize and scale the variables in the dataset:

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

Standardized and scaled variables now have negative values as before there were only positive values. The values are now in "the same ball park" for all the variables.


Here we create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate) using quantiles (and renaming them "low", "med_low", "med_high", "high") and we drop the original crime rate variable out: 

```{r}
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

Here we divide the dataset used for training (80% of data) and using to test (20% of the data):

```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

```

## 5. Fitting the linear discriminant analysis on the train set

Here we fit the linear discriminant analysis (LDA) on the training set. The previously modified categorical crime rate ("low", "med_low", "med_high", "high") is used as target variable and everything else is used as predictors.

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

```

Let's draw the LDA (bi)plot: 

```{r}
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```

Seems that most of the high crime rate is best predicted by **rad** - *index of accessibility to radial highways*. 
**zn** - *proportion of residential land zoned for lots over 25,000 sq.ft.* seems to best predict low crime rates. So a defiency of small houses predicts lower crime rate...

**nox** - *nitrogen oxides concentration (parts per 10 million).*

## 6. Predicting classes with the LDA model on the test data

```{r}
# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)

```
High crime rates especially seem to be most correctly predicted using the training data. Also low and med_low are predicted correctly most of the cases. The weakest prediction is for med_low crime rate. Note that the sampling (where we choose 80% for training) might have an effect on how well the med_low and med_high are predicted as their prediction is not as "easy" as for low and high crime rate. The more of a cohesive cluster of the target variable (for example in the previous plot) the more easier it is to predict.

## 7. Standardizing data with comparable distances... and k-means

Here we standardize the data with comparable distances and calculate the distances between the observations.

```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)
```
It is somewhat unclear whether to explore the k-means algorithm with the SCALED data... but here we go:

```{r}
# fig.height = 19, fig.width = 17}

km <-kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
```

#Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results


```{r}
# fig.height = 19, fig.width = 17}

set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)

```

